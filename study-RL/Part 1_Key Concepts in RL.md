> í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ì¸ê³µì§€ëŠ¥ ë°ë¸Œì½”ìŠ¤ 6ê¸° ê°•í™”í•™ìŠµ ìŠ¤í„°ë””  
[[Open AI spinning up] Introduction to RL - Part 1: Key Concepts in RL]([https://spinningup.openai.com/en/latest/spinningup/rl_intro.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html))

# Part 1: Key Concepts in RL


ê°•í™”í•™ìŠµì€ agentê°€ ì£¼ì–´ì§„ í™˜ê²½(environment)ì—ì„œ ì–´ë–»ê²Œ ì‹œí–‰ì°©ì˜¤(trial and error)ë¥¼ í†µí•´ í•™ìŠµí•˜ëŠ”ì§€ì— ëŒ€í•œ ì—°êµ¬ì´ë‹¤.

## Key Concepts and Terminology

![ì—ì´ì „íŠ¸-í™˜ê²½ ìƒí˜¸ì‘ìš© ë£¨í”„](https://spinningup.openai.com/en/latest/_images/rl_diagram_transparent_bg.png)

ì—ì´ì „íŠ¸-í™˜ê²½ ìƒí˜¸ì‘ìš© ë£¨í”„

ê°•í™”í•™ìŠµì˜ ì£¼ì¸ê³µì€ **ì—ì´ì „íŠ¸**(agent)ì™€ **í™˜ê²½**(environment)ì´ë‹¤. í™˜ê²½ì€ ì—ì´ì „íŠ¸ê°€ ì‚´ê³  ìƒí˜¸ì‘ìš©í•˜ëŠ” ì„¸ê³„ì´ë‹¤. ì—ì´ì „íŠ¸ëŠ” í™˜ê²½ìœ¼ë¡œë¶€í„° **ë¦¬ì›Œë“œ**(reward)ë¥¼ ë°›ëŠ”ë‹¤. ë¦¬ì›Œë“œë€ í˜„ì¬ì˜ ìƒíƒœê°€ ì–¼ë§ˆë‚˜ ì¢‹ê³  ë‚˜ìœì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ì²™ë„ì´ë‹¤. ì—ì´ì „íŠ¸ì˜ ëª©í‘œëŠ” ëˆ„ì  ë¦¬ì›Œë“œ **ë¦¬í„´**(return)ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ë‹¤.

ê°•í™”í•™ìŠµì€ ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” í–‰ë™ì„ ë°°ìš°ëŠ” ë°©ë²•ì´ë‹¤. 

## Terminology

### States and Observations

- **state** *s*: worldì— ëŒ€í•œ complete description
- **observation** *o*: stateì— ëŒ€í•œ ë¶€ë¶„ì ì¸ description

â†’ deep RLì—ì„œëŠ” stateì™€ observationsë¥¼ ë²¡í„°, í–‰ë ¬, ê³ ì°¨í…ì„œë¡œ ë‚˜íƒ€ë‚¸ë‹¤.

ğŸ’¡ì—ì´ì „íŠ¸ëŠ” ì„¸ê³„ì— ëŒ€í•œ ë¶€ë¶„ì ì¸ ì •ë³´ë¥¼ ê°€ì§€ê³  í–‰ë™ì„ ê²°ì •í•˜ê¸° ë•Œë¬¸ì— observationë¥¼ ì“°ëŠ” ê²Œ ë” ì •í™•í•œ ìƒí™©ì—ì„œë„ ì¼ë°˜ì ìœ¼ë¡œ stateë¼ê³  ë§ì´ ì“´ë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œë„ standard conventionì„ ë”°ë¼ së¼ê³  ì“°ë„ë¡ í•˜ê² ë‹¤.

### Action Spaces

ë‹¤ë¥¸ í™˜ê²½ë“¤ì€ ê°ì ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê°€ëŠ¥í•œ actionì´ ìˆë‹¤. ì£¼ì–´ì§„ í™˜ê²½ì—ì„œ ê°€ëŠ¥í•œ actionë“¤ì˜ ì§‘í•©ì„ **action space**ë¼ê³  í•œë‹¤.

- discrete action spaces: Atrai, ë°”ë‘‘ê³¼ ê°™ì´ í•œì •ëœ actionì´ ê°€ëŠ¥í•œ ê²½ìš°
- continuous action spaces: ë¬¼ë¦¬ì  ì„¸ê³„ì˜ ë¡œë´‡ì„ ì¡°ì¢…í•˜ëŠ” ê²ƒê³¼ ê°™ì´ actionì„ ì‹¤ìˆ˜ ë²¡í„°ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê²½ìš°

### Policies

ì—ì´ì „íŠ¸ê°€ actionì„ ê²°ì •í•˜ëŠ” ruleì„ **policy**ë¼ê³  í•œë‹¤.

- deterministic (ê²°ì •ì /í™•ì •ì ) : $\mu$ë¥¼ ì´ìš©í•˜ì—¬ í‘œí˜„
- stochastic (í™•ë¥ ì ): $\pi$ë¥¼ ì´ìš©í•˜ì—¬ í‘œí˜„
    
    ![https://spinningup.openai.com/en/latest/_images/math/831f731859658682b2af7e217a76648697c9de46.svg](https://spinningup.openai.com/en/latest/_images/math/831f731859658682b2af7e217a76648697c9de46.svg)
    

*(â†’ í˜„ì¬ì˜ ìƒí™©ì„ policy í•¨ìˆ˜ì— ë„£ìœ¼ë©´ ì·¨í•´ì•¼ í•  actionì´ ë‚˜ì˜¨ë‹¤)*

ì´ëŸ¬í•œ policyëŠ” ìš°ë¦¬ê°€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ë™ì‘ì„ ë³€ê²½í•  ìˆ˜ ìˆë„ë¡ íŒŒë¼ë¯¸í„°ì— ì˜ì¡´ì ì¸ ê³„ì‚° ê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ë‹¤ë¤„ì§„ë‹¤. 

**Stochastic Policies**

stochastic policies ì¤‘ ê°€ì¥ ì¼ë°˜ì ì¸ ì¢…ë¥˜ë¡œëŠ” discrete action spaceì—ì„œ ì“°ëŠ” **categorical policies**, continuous action spacesì—ì„œ ì“°ëŠ” **diagonal Gaussian policies**ê°€ ìˆë‹¤.

stochastic policyì˜ key computation ë‘ ê°€ì§€

1) policyë¡œë¶€í„° actionì„ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒ 

2) íŠ¹ì • actionì˜ log likelihoodë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒ 

### Trajectories

ì—ì´ì „íŠ¸ê°€ ê±°ì¹œ ëª¨ë“  stateì™€ actionì˜ ê¶¤ì ìœ¼ë¡œ, episode, rolloutì´ë¼ê³  ë¶ˆë¦¬ê¸°ë„ í•œë‹¤.

![https://spinningup.openai.com/en/latest/_images/math/8337d86159a1cd98dfcd0601993d7b6b2fbb54d9.svg](https://spinningup.openai.com/en/latest/_images/math/8337d86159a1cd98dfcd0601993d7b6b2fbb54d9.svg)

$s_0$ì€ start-state distribution $\rho_0$ì— ì˜í•´ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§ëœë‹¤. 

![https://spinningup.openai.com/en/latest/_images/math/eef23a6502b9cec4bc399bcbce93547c3723643c.svg](https://spinningup.openai.com/en/latest/_images/math/eef23a6502b9cec4bc399bcbce93547c3723643c.svg)

tì—ì„œì˜ ìƒíƒœ $s_t$ì—ì„œ t+1ì˜ ìƒíƒœ  $s_t+1$ë¡œì˜ state transitionì€ í™˜ê²½ê³¼ ê°€ì¥ ìµœê·¼ì˜ action $a_t$ì— ì˜í•´ ê²°ì •ë˜ë©° ê²°ì •ì ì´ê±°ë‚˜ í™•ë¥ ì ìœ¼ë¡œ ê²°ì •ëœë‹¤. 

![https://spinningup.openai.com/en/latest/_images/math/16da6346104894fb6a673473cbfc9ffeba6471fa.svg](https://spinningup.openai.com/en/latest/_images/math/16da6346104894fb6a673473cbfc9ffeba6471fa.svg)

![https://spinningup.openai.com/en/latest/_images/math/872390af4f5b2541d17e7ef2bfaecbe1e9746d94.svg](https://spinningup.openai.com/en/latest/_images/math/872390af4f5b2541d17e7ef2bfaecbe1e9746d94.svg)

ì´ ë•Œ ì—ì´ì „íŠ¸ì˜ actionì€ policyì— ì˜í•´ ê²°ì •ëœë‹¤.

### Reward and Return

ë¦¬ì›Œë“œí•¨ìˆ˜ Rì€ í˜„ì¬ state, ë°©ê¸ˆ í•œ action, ë‹¤ìŒ stateì— ì˜ì¡´í•œë‹¤. (í˜„ì¬ í–‰ë™ì˜ ë³´ìƒì¸ë“¯)

![https://spinningup.openai.com/en/latest/_images/math/6ed565b0911f12c8ef64d93a617d8bb30380d5d5.svg](https://spinningup.openai.com/en/latest/_images/math/6ed565b0911f12c8ef64d93a617d8bb30380d5d5.svg)

ì—ì´ì „íŠ¸ì˜ ëª©ì ì€ trajectoryë¥¼ ë”°ë¥¸ ëˆ„ì  ë¦¬ì›Œë“œ returnì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ë‹¤. returnì€ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤.

1) finite-horizon undiscounted return

- íŠ¹ì • ê¸°ê°„ë™ì•ˆ ë‹¨ìˆœí•œ ë¦¬ì›Œë“œì˜ í•©

![https://spinningup.openai.com/en/latest/_images/math/b2466507811fc9b9cbe2a0a51fd36034e16f2780.svg](https://spinningup.openai.com/en/latest/_images/math/b2466507811fc9b9cbe2a0a51fd36034e16f2780.svg)

2) infinite-horizon discounted return

- ëª¨ë“  ê¸°ê°„ë™ì•ˆì˜ ë¦¬ì›Œë“œì˜ discountedëœ í•©(discount factorì™€ì˜ ê°€ì¤‘í•©)
- discountì˜ ì˜ë¯¸ëŠ” 1) ìˆ˜ë ´ì„ ìœ„í•´ 2) í˜„ì¬ì˜ ê°€ì¹˜ì— ë” ê°€ì¤‘ì¹˜ë¥¼ ë‘ê¸° ìœ„í•´

![https://spinningup.openai.com/en/latest/_images/math/bf49428c66c91a45d7b66a432450ee49a3622348.svg](https://spinningup.openai.com/en/latest/_images/math/bf49428c66c91a45d7b66a432450ee49a3622348.svg)

### The RL Problem

ê°•í™”í•™ìŠµì˜ ëª©ì ì€ **expected return**ì„ ìµœëŒ€í™”í•˜ëŠ” policyë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. 

í™˜ê²½ ì „ì´ì™€ policyê°€ ëª¨ë‘ í™•ë¥ ì (stochastic)í•œ ìƒí™©ì´ë¼ê³  ê°€ì •í–ˆì„ ë•Œ, T-step trajectoryì˜ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

![https://spinningup.openai.com/en/latest/_images/math/69369e7fae3098a2f05a79680fbecbf48a4e7f66.svg](https://spinningup.openai.com/en/latest/_images/math/69369e7fae3098a2f05a79680fbecbf48a4e7f66.svg)

ì´ ë•Œì˜ expected return í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê° stepì—ì„œì˜ ë¦¬ì›Œë“œì˜ ê¸°ëŒ“ê°’ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

![https://spinningup.openai.com/en/latest/_images/math/f0d6e3879540e318df14d2c8b68af828b1b350da.svg](https://spinningup.openai.com/en/latest/_images/math/f0d6e3879540e318df14d2c8b68af828b1b350da.svg)

ì´ ë•Œ ê°•í™”í•™ìŠµì˜ ì¤‘ì‹¬ ìµœì í™” ë¬¸ì œ(centralized optimization problem)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

![https://spinningup.openai.com/en/latest/_images/math/2de61070bf8758d03104b4f15df45c8ff5a86f5a.svg](https://spinningup.openai.com/en/latest/_images/math/2de61070bf8758d03104b4f15df45c8ff5a86f5a.svg)

*(expected returnì„ ìµœëŒ€í™”í•˜ëŠ” policyë¥¼ ì°¾ëŠ” ê²ƒ)*

ì´ ë•Œì˜ $\pi^*$ë¥¼ **optimal policy**ë¼ê³  í•œë‹¤.

### Value Function

ì§€ê¸ˆ stateë‚˜ state-action pairì—ì„œ ì‹œì‘í•˜ì—¬ íŠ¹ì • policyë¥¼ ë”°ë¼ ì´ë™í–ˆì„ ë•Œì˜ **expected return**ì„ë¥¼ valueë¼ê³  í•œë‹¤.

- On-policy Value Function
    - íŠ¹ì • state sì—ì„œ ì–´ë–¤ ì •ì±… $\pi$ë¥¼ ë”°ë¼ì„œ ëê¹Œì§€ ì›€ì§ì˜€ì„ ë•Œì˜ expected return
        
        ![https://spinningup.openai.com/en/latest/_images/math/e043709b46c9aa6811953dabd82461db6308fe19.svg](https://spinningup.openai.com/en/latest/_images/math/e043709b46c9aa6811953dabd82461db6308fe19.svg)
        
- On-Policy Action-Value Function
    - íŠ¹ì • state sì—ì„œ ì„ì˜ì˜ action aë¥¼ ì‹œí–‰í•˜ê³ , ê·¸ í›„ ì–´ë–¤ ì •ì±… $\pi$ë¥¼ ë”°ë¼ ëê¹Œì§€ ì›€ì§ì˜€ì„ ë•Œì˜ expected return
        
        ![https://spinningup.openai.com/en/latest/_images/math/85d41c8c383a96e1ed34fc66f14abd61b132dd28.svg](https://spinningup.openai.com/en/latest/_images/math/85d41c8c383a96e1ed34fc66f14abd61b132dd28.svg)
        
- Optimal Value Function
    - íŠ¹ì • state sì—ì„œ optimal policyë¥¼ ë”°ë¼ì„œ ëê¹Œì§€ ì›€ì§ì˜€ì„ ë•Œì˜ expected return
        
        ![https://spinningup.openai.com/en/latest/_images/math/01d48ea453ecb7b560ea7d42144ae24422fbd0eb.svg](https://spinningup.openai.com/en/latest/_images/math/01d48ea453ecb7b560ea7d42144ae24422fbd0eb.svg)
        
- Optimal Action-Value Function
    - íŠ¹ì • state sì—ì„œ ì„ì˜ì˜ action aë¥¼ ì‹œí–‰í•˜ê³ , ê·¸ í›„ optimal policyë¥¼ ë”°ë¼ ëê¹Œì§€ ì›€ì§ì˜€ì„ ë•Œì˜ expected return
        
        ![https://spinningup.openai.com/en/latest/_images/math/bc92e8ce1cf0aaa212e144d5ed74e3b115453cb6.svg](https://spinningup.openai.com/en/latest/_images/math/bc92e8ce1cf0aaa212e144d5ed74e3b115453cb6.svg)
        

### The Optimal Q-Function and the Optimal Action

optimal action-value function $Q^*(s,a)$ì€ state sì—ì„œ ì‹œì‘í•´ì„œ ì„ì˜ì˜ í–‰ë™ aë¥¼ í•˜ê³  ë‚˜ì„œ optimal policyë¥¼ ë”°ëì„ ë•Œì˜ expected returnì„ ì•Œë ¤ì¤€ë‹¤. ë”°ë¼ì„œ $Q^*$ë¥¼ ì•Œë©´ ë‹¤ìŒê³¼ ê°™ì´ optimal action $a^*(s)$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

![https://spinningup.openai.com/en/latest/_images/math/42490c4d812c9ca1fc226684577900bc8bdd609b.svg](https://spinningup.openai.com/en/latest/_images/math/42490c4d812c9ca1fc226684577900bc8bdd609b.svg)

### Bellman Equations

ë²¨ë§Œ ë°©ì •ì‹ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” **â€œì‹œì‘ì ì˜ valueëŠ” í˜„ì¬ stateì—ì„œì˜ ë¦¬ì›Œë“œì™€ ë‹¤ìŒ stateì—ì„œì˜ valueë¥¼ í•©í•œ ê²ƒê³¼ ê°™ë‹¤â€**ëŠ” ê²ƒì´ë‹¤.

![https://spinningup.openai.com/en/latest/_images/math/7e4a2964e190104a669406ca5e1e320a5da8bae0.svg](https://spinningup.openai.com/en/latest/_images/math/7e4a2964e190104a669406ca5e1e320a5da8bae0.svg)

*($s'$ëŠ” ë‹¤ìŒ stateë¥¼ ëœ»í•œë‹¤)*

*â†’ ìœ„ì—ì„œì˜ value functionì„ í˜„ì¬ ìƒíƒœì˜ ë¦¬ì›Œë“œ + ë‹¤ìŒ ìƒíƒœì—ì„œì˜ valueë¡œ ë‚˜ëˆˆ ê²ƒ!*

![https://spinningup.openai.com/en/latest/_images/math/f8ab9b211bc9bb91cde189360051e3bd1f896afa.svg](https://spinningup.openai.com/en/latest/_images/math/f8ab9b211bc9bb91cde189360051e3bd1f896afa.svg)

\* Bellman backup: ë²¨ë§Œ ë°©ì •ì‹ì˜ ìš°í•­, í˜„ì¬ ë¦¬ì›Œë“œ + ë‹¤ìŒ value

### Advantage Functions

ê°•í™”í•™ìŠµì—ì„œëŠ” ì–´ë–¤ actionì´ ì ˆëŒ€ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë³´ë‹¤, ë‹¤ë¥¸ actionë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ê°€ ì¤‘ìš”í•  ë•Œê°€ ìˆë‹¤. **advantage function**ì€ state sì—ì„œ policy $\pi$ë¥¼ ë”°ë¼ íŠ¹ì • action aë¥¼ í–ˆì„ ë•Œ $\pi(*|s)$ë¥¼ ë”°ë¼ ëœë¤í•œ actionì„ í–ˆì„ ë•Œ ì–¼ë§ˆë‚˜ ë‚˜ì€ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤.

![https://spinningup.openai.com/en/latest/_images/math/3748974cc061fb4065fa46dd6271395d59f22040.svg](https://spinningup.openai.com/en/latest/_images/math/3748974cc061fb4065fa46dd6271395d59f22040.svg)

### Formalism

**Markov Decision Processes(MDP)**

MDPëŠ” ì‹œìŠ¤í…œì´ Markov propertyë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì„ ëœ»í•œë‹¤. ë§ˆë¥´ì½”í”„ ì†ì„±ì€ transitionì´ ì´ì „ì´ ì•„ë‹Œ, ê°€ì¥ ìµœê·¼ì˜ stateì™€ actionì—ë§Œ ì˜ì¡´í•œë‹¤ëŠ” ê²ƒì´ë‹¤. MDPëŠ” $<S, A, R, P, \rho_0>$ë¼ëŠ” 5-íŠœí”Œë¡œ í‘œí˜„ëœë‹¤.

$S$: ê°€ëŠ¥í•œ ëª¨ë“  stateì˜ ì§‘í•©

$A$: ê°€ëŠ¥í•œ ëª¨ë“  actionì˜ ì§‘í•©

$R$: ë¦¬ì›Œë“œ í•¨ìˆ˜

$P$: transition probability function

$\rho_0$: starting state distribution